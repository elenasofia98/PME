{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46912e-c03d-4cdf-b0aa-c54ae2d71a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48c0cf-ac79-4418-abb8-3ed277f2162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the folder you want to import from\n",
    "sys.path.append(os.path.abspath('../EasyEdit/'))\n",
    "\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "MULTI = True\n",
    "\n",
    "if not MULTI:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= f\"{DEVICE_NUM}\" # '' #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a6e1a-b54a-4e56-a36b-bfaeabae78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d12f28-187b-4af3-94fb-3c491392fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_type = 'pii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fbc11-c1e1-41ce-8abc-1173e9163d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "\n",
    "\n",
    "#device = f\"cuda:{DEVICE_NUM}\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_type = 'gpt-j' #'gpt-neo' #\n",
    "models = ['6B'] #['1.3B', '2.7B'] #\n",
    "model_size = models[0]\n",
    "\n",
    "\n",
    "if model_type == 'gpt-j':\n",
    "    model_name = f\"EleutherAI/gpt-j-{model_size}\"\n",
    "elif model_type == 'gpt-neo':\n",
    "    model_name = f\"EleutherAI/gpt-neo-{model_size}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa0f4e-fb5d-4feb-99f3-d37f640fe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{model_type}-{model_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39aab1-aade-4e95-a7f2-9e74de55e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2adb7-28de-4653-bcf3-16203e5f5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = 200\n",
    "\n",
    "UPDATE_METHODS = ['pre_edit', f'memoedit-{CONTEXT}', f'MEMIT-{CONTEXT}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770b2f2-4eb7-4cda-9ee7-8042f53c8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval.models.huggingface import HFLM\n",
    "from lm_eval import tasks, evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccce072-d681-4f20-9087-135600072b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the task you want to evaluate on\n",
    "TASKS = [\"hellaswag\", \"lambada\", 'wikitext', 'winogrande', 'piqa']  \n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28592bc3-a25b-4840-b10e-0cfc9c8ce544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a14ee-386b-44ca-b071-40b8186eab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "redo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c6786-f75d-4fc0-aa92-d9fdc5bda98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "\n",
    "    \n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as pickle_handler:\n",
    "        results = pickle.load(pickle_handler)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da59a90-6c25-4290-8466-461652d3f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"lm-eval-harness-res-{model_type}-{model_size}-{pii_type}\"):\n",
    "    os.mkdir(f\"lm-eval-harness-res-{model_type}-{model_size}-{pii_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e093707-0062-47b0-abce-3078b15b7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef6d78-44f8-42e7-8bae-fc3cd1b65693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor.models.grace.GRACE import GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e386b6-6cb7-4a3c-9ce6-df2879b70b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from easyeditor.models.grace.grace_hparams import GraceHyperParams\n",
    "#from easyeditor.models.grace.GRACE import GRACEAdapter\n",
    "#torch.serialization.add_safe_globals([GraceHyperParams, GRACEAdapter])\n",
    "#torch.serialization.safe_globals([GraceHyperParams, GRACEAdapter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b699a2-965e-43ef-85d0-37d95d6b8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_METHODS[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027a7e7-a6ec-4b8a-84c0-a3ccd896e728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177e782-4901-4ad0-9cad-a346cc3f31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for UPDATE_METHOD in UPDATE_METHODS:\n",
    "    print('*'*80)\n",
    "    print(UPDATE_METHOD)\n",
    "    print('*'*80)\n",
    "\n",
    "    filename = f'./lm-eval-harness-res-{model_type}-{model_size}-{pii_type}/{UPDATE_METHOD}.pkl'\n",
    "    if not redo and os.path.exists(filename):\n",
    "        print(\"*\"*80)\n",
    "        print(\"ATTENZIONE GIA' CALCOLATO\")\n",
    "        print(\"Generazione al momento saltata\")\n",
    "        print(\"*\"*80)\n",
    "        print()\n",
    "        scores[UPDATE_METHOD] = load_pickle(filename)\n",
    "        continue\n",
    "\n",
    "    scores[UPDATE_METHOD] = {}  \n",
    "    if UPDATE_METHOD == 'pre_edit':\n",
    "        model_path = model_name\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    else:\n",
    "        if UPDATE_METHOD.startswith(\"memoedit\") or UPDATE_METHOD.startswith(\"MEMIT\"):\n",
    "            BATCH_SIZE = {'memoedit-200':8, 'MEMIT-200':8}[UPDATE_METHOD] # TODO da specificare a mano per ora\n",
    "            model_path = f\"../EasyEdit/edited_states_{model_type}-{model_size}/{UPDATE_METHOD.replace('-', '_')}_{BATCH_SIZE}_{pii_type}_all_edited_states.pt\"\n",
    "        #elif UPDATE_METHOD.startswith('dememorize'):\n",
    "        #    model_path = f\"../DeMemorization-main/{UPDATE_METHOD}_{model_type}-{model_size}_{pii_type}\"\n",
    "        else:\n",
    "            model_path = f\"../EasyEdit/edited_states_{model_type}-{model_size}/{UPDATE_METHOD.replace('-', '_')}_all_edited_states.pt\"\n",
    "        \n",
    "        print(model_path)\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(\"Edited states not computed, skipped!\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if not UPDATE_METHOD.startswith('dememorize') and not UPDATE_METHOD.startswith('GRACE'):\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "            \n",
    "            #model = model.to(device)\n",
    "            \n",
    "            edited_layes = torch.load(model_path)#, map_location='auto')#torch.device(device))\n",
    "            edited_states = model.state_dict()\n",
    "            \n",
    "            for i in edited_layes.keys():\n",
    "                edited_states[f\"{i}.weight\"] = edited_layes[i]\n",
    "            \n",
    "            model.load_state_dict(edited_states)\n",
    "            del edited_states\n",
    "        elif UPDATE_METHOD.startswith('GRACE'):\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "            model = model.to(device)\n",
    "            model = GRACE.from_pretrained(\n",
    "                model=model,\n",
    "                device=device,\n",
    "                adapter_ckpt_path=model_path,\n",
    "                weights_only=False\n",
    "            )\n",
    "            model.activate_inference_state()\n",
    "            model=model.model\n",
    "        \n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "            \n",
    "    #model = model.to(device)\n",
    "    model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "\n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    display(model)\n",
    "    \n",
    "    # Load model wrapper\n",
    "    model = HFLM(pretrained=model, \n",
    "                 tokenizer=tokenizer, batch_size=8)\n",
    "\n",
    "    # Compute scores\n",
    "    for task in TASKS:\n",
    "        results = evaluator.simple_evaluate(\n",
    "            model=model,\n",
    "            tasks=[task],\n",
    "            num_fewshot=0,\n",
    "            limit=500,\n",
    "            bootstrap_iters=500 if model_size!='6B' else 50,\n",
    "            max_batch_size=32 if model_size!='6B' else 4\n",
    "        )\n",
    "\n",
    "        for t in results['results']: \n",
    "            scores[UPDATE_METHOD][t] = results['results'][t]\n",
    "    \n",
    "    model._model = model._model.to('cpu')\n",
    "    del model._model\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with open(filename, \"wb\") as pickle_handler:\n",
    "        pickle.dump(scores[UPDATE_METHOD], pickle_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e165f-dd57-44d3-858e-61fe874efa45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123e5be-ea1a-47a1-aacd-58b48befdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print_scores = {}\n",
    "for UPDATE_METHOD in scores:\n",
    "    for t in scores[UPDATE_METHOD]:\n",
    "        if t not in to_print_scores:\n",
    "            to_print_scores[t] = {}\n",
    "        to_print_scores[t][UPDATE_METHOD] = scores[UPDATE_METHOD][t]\n",
    "        to_print_scores[t][UPDATE_METHOD]['method'] = UPDATE_METHOD\n",
    "\n",
    "to_print_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13870bd-42db-4b18-bd20-0294121b15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = to_print_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a03bf-6023-4309-a674-bd46b628cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for t in scores:\n",
    "    df = pd.DataFrame([scores[t][UPDATE_METHOD] for UPDATE_METHOD in scores[t]])\n",
    "    display(df)\n",
    "    df.to_csv(f'lm-eval-harness-res-{model_type}-{model_size}-{pii_type}/{t}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff1f8b-fef1-430a-8dbe-d84c41a42d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ceda7c-af59-456d-87da-f5a39512e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for t in scores:\n",
    "    df = pd.DataFrame([scores[t][UPDATE_METHOD] for UPDATE_METHOD in scores[t]])\n",
    "    df = df.rename(columns=lambda x: x if ',none' not in x else x.split(',none')[0])\n",
    "    \n",
    "    metric = df.columns[1]\n",
    "    df[f'{metric}_stderr'] = df[f'{metric}_stderr'].replace('N/A', 0)\n",
    "    \n",
    "    display(df[[metric, f'{metric}_stderr', 'method']])\n",
    "    dfs[t] = df[[metric, f'{metric}_stderr', 'method']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997e808-a388-4688-bdb1-59a0a307538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_method_names(values):\n",
    "    new_names = []\n",
    "    mapping = {\n",
    "        'pre_edit': 'Pre Edit',\n",
    "    f'memoedit-{CONTEXT}': 'PME', \n",
    "    f'MEMIT-{CONTEXT}': 'MEMIT', \n",
    "    f'dememorize-{CONTEXT}': 'DeMem',\n",
    "    f'GRACE-{CONTEXT}': 'GRACE'\n",
    "    }\n",
    "    return [mapping[v] for v in values]\n",
    "\n",
    "def rename_metric(x):\n",
    "    m = {\n",
    "        'acc': 'Accuracy↑',\n",
    "        'perplexity': 'Perplexity↓',\n",
    "        'word_perplexity': 'Word Perplexity↓'\n",
    "    }\n",
    "    return m[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8c479-1553-4543-b6f3-25fb4e03b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "        'pre_edit': 'Pre Edit',\n",
    "    f'memoedit-{CONTEXT}': 'PME', \n",
    "    f'MEMIT-{CONTEXT}': 'MEMIT', \n",
    "    f'dememorize-{CONTEXT}': 'DeMem',\n",
    "    f'GRACE-{CONTEXT}': 'GRACE'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "to_print_scores = {}\n",
    "\n",
    "for t in dfs:\n",
    "    to_print_scores[t] = dfs[t].set_index('method')\n",
    "    metric = to_print_scores[t].columns[0]\n",
    "    to_print_scores[t][metric] = [f'{round(v, 2)}({round(s,2)})' for v, s in zip(to_print_scores[t][metric], to_print_scores[t][f'{metric}_stderr'])]\n",
    "    to_print_scores[t] = to_print_scores[t][[metric]].T\n",
    "\n",
    "to_print_scores = pd.concat(to_print_scores)\n",
    "to_print_scores = to_print_scores.rename(columns= lambda x: mapping[x])\n",
    "to_print_scores.index = [ f\"{t.replace('_', ' ').capitalize()} {rename_metric(m)}\"\n",
    "    for t, m in to_print_scores.index\n",
    "]\n",
    "to_print_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c9caa-9529-4f86-b89f-19887d2d6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import to_hex, LinearSegmentedColormap\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "dataframes = []\n",
    "tasks = []\n",
    "\n",
    "for k, df in dfs.items():\n",
    "    dataframes.append(df)\n",
    "    tasks.append(k)\n",
    "\n",
    "forest_green_gradient = LinearSegmentedColormap.from_list(\n",
    "    \"forest_green_gradient\", [\"#a8e6a3\", \"#1b5e20\"]\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(dataframes), figsize=(len(dataframes)*3, 3), sharey=False)\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    ax = axes[i]\n",
    "    methods = map_method_names(df['method'].values)\n",
    "    metrics = df[df.columns[0]]\n",
    "    stds = df[df.columns[1]]\n",
    "\n",
    "    # Generate colors based on the number of methods\n",
    "    num_methods = len(methods)\n",
    "    colors = [to_hex(forest_green_gradient(j / (num_methods - 1))) for j in range(num_methods)]\n",
    "\n",
    "    # Plot bars with gradient colors\n",
    "    ax.bar(methods, metrics, capsize=5, color=colors, alpha=0.9)\n",
    "\n",
    "    ax.set_title(f\"{tasks[i].replace('_', ' ').capitalize()}\", fontsize=16)\n",
    "\n",
    "    # Custom limits and y-ticks\n",
    "    y_min = (metrics - stds).min() if sum(stds)!=0 else (metrics - 0.1).min()\n",
    "    y_max = (metrics + 1.2*stds).max() if sum(stds)!=0 else (metrics + 0.1).max()\n",
    "    y_padding = (y_max - y_min) * 0.1\n",
    "\n",
    "    ax.set_ylim(y_min - y_padding, y_max + y_padding)\n",
    "\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(nbins=6))\n",
    "    ax.set_ylabel(rename_metric(df.columns[0]), fontsize=16)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'lm-eval-harness-res-{model_type}-{model_size}-{pii_type}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c178a45-69b3-4f71-8a01-322ec21df876",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d492cc-f7d4-4aac-a8bb-09e12a9df5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163ebee-311e-4c69-afee-281695635b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2ff5c-dea3-44a9-bea4-bce54d36e0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3867838-e016-48d4-a7de-8932f3eaf954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b123e2b-da00-41fc-8a0c-5cc6e2bb3643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5b581236-f6a7-4d05-85b0-ed0e40893e59",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "scores = {}\n",
    "for UPDATE_METHOD in UPDATE_METHODS:\n",
    "    print('*'*80)\n",
    "    print(UPDATE_METHOD)\n",
    "    print('*'*80)\n",
    "\n",
    "    filename = f'./lm-eval-harness-res-{model_type}-{model_size}-{pii_type}/{UPDATE_METHOD}.pkl'\n",
    "    if not redo and os.path.exists(filename):\n",
    "        print(\"*\"*80)\n",
    "        print(\"ATTENZIONE GIA' CALCOLATO\")\n",
    "        print(\"Generazione al momento saltata\")\n",
    "        print(\"*\"*80)\n",
    "        print()\n",
    "        scores[UPDATE_METHOD] = load_pickle(filename)\n",
    "        continue\n",
    "        \n",
    "    scores[UPDATE_METHOD] = {}    \n",
    "    if UPDATE_METHOD == 'pre_edit':\n",
    "        model_path = model_name\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    else:\n",
    "        if UPDATE_METHOD.startswith(\"memoedit\") or UPDATE_METHOD.startswith(\"MEMIT\"):\n",
    "            BATCH_SIZE = {'memoedit-200':8, 'MEMIT-200':8}[UPDATE_METHOD] # TODO da specificare a mano per ora\n",
    "            model_path = f\"../EasyEdit/edited_states_{model_type}-{model_size}/{UPDATE_METHOD.replace('-', '_')}_{BATCH_SIZE}_{pii_type}_all_edited_states.pt\"\n",
    "        else:\n",
    "            model_path = f\"../EasyEdit/edited_states_{model_type}-{model_size}/{UPDATE_METHOD.replace('-', '_')}_{pii_type}_all_edited_states.pt\"\n",
    "        \n",
    "        print(model_path)\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(\"Edited states not computed, skipped!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if UPDATE_METHOD!='MEND':\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "            \n",
    "            model = model.to(device)\n",
    "            \n",
    "            edited_layes = torch.load(model_path, map_location=torch.device(device))\n",
    "            edited_states = model.state_dict()\n",
    "            \n",
    "            for i in edited_layes.keys():\n",
    "                edited_states[f\"{i}.weight\"] = edited_layes[i]\n",
    "                \n",
    "            model.load_state_dict(edited_states)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "            \n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    display(model)\n",
    "\n",
    "    # Load model wrapper\n",
    "    model = HFLM(\n",
    "        pretrained=model, \n",
    "        tokenizer=tokenizer, \n",
    "        # batch_size=8 #==> to test, possible issues, see: https://github.com/EleutherAI/lm-evaluation-harness/issues/626\n",
    "        max_batch_size=32, # this parameter has priority over evaluator.simple_evaluate \n",
    "        max_memory_per_gpu=40 # experimental parameter to limit the GPU memory usage\n",
    "    )\n",
    "\n",
    "    # Compute scores\n",
    "    for task in TASKS:\n",
    "        print(\"-\"*50)\n",
    "        print(task)\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        results = evaluator.simple_evaluate(\n",
    "            model=model,\n",
    "            tasks=[task],\n",
    "            num_fewshot=0,\n",
    "            limit=500,\n",
    "            bootstrap_iters=1000,\n",
    "            max_batch_size=16\n",
    "        )\n",
    "\n",
    "        for t in results['results']: \n",
    "            scores[UPDATE_METHOD][t] = results['results'][t]\n",
    "    \n",
    "    model._model = model._model.to('cpu')\n",
    "    del model._model\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with open(filename, \"wb\") as pickle_handler:\n",
    "        pickle.dump(scores[UPDATE_METHOD], pickle_handler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
